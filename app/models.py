from typing import Optional, Dict
import torch
import sys
import time
import os
import threading
import logging
from cosyvoice.cli.cosyvoice import AutoModel

from .config import settings, VoiceConfig

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ========== å…¨å±€å˜é‡ ==========
cosy_model: Optional[AutoModel] = None
voice_cache_manager: Optional['VoiceCacheManager'] = None
inference_lock = threading.Lock()  # å¹¶å‘æ¨ç†ä¿æŠ¤é”


class VoiceCacheManager:
    """éŸ³è‰²ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, model: AutoModel):
        self.model = model
        self.voice_cache: Dict[str, Dict] = {}
        self.default_voice_id = settings.DEFAULT_VOICE_ID
    
    def load_voices(self) -> int:
        """
        æ‰¹é‡åŠ è½½éŸ³è‰²é…ç½®
        
        Returns:
            æˆåŠŸåŠ è½½çš„éŸ³è‰²æ•°é‡
        """
        logger.info(f"âš¡ æ­£åœ¨åŠ è½½ {len(settings.VOICE_CONFIGS)} ä¸ªéŸ³è‰²é…ç½®...")
        
        loaded_count = 0
        for voice_config in settings.VOICE_CONFIGS:
            if self._load_single_voice(voice_config):
                loaded_count += 1
        
        logger.info(f"âš¡ éŸ³è‰²åŠ è½½å®Œæˆ,å…± {loaded_count} ä¸ªå¯ç”¨éŸ³è‰²: {list(self.voice_cache.keys())}")
        return loaded_count
    
    def _load_single_voice(self, voice_config: Dict) -> bool:
        """
        åŠ è½½å•ä¸ªéŸ³è‰²
        
        Args:
            voice_config: éŸ³è‰²é…ç½®å­—å…¸
        
        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        voice_id = voice_config["id"]
        voice_file = voice_config["file"]
        prompt_text = voice_config["prompt_text"]
        description = voice_config.get("description", "")
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        voice_path = os.path.join(settings.ASSET_DIR, voice_file)
        
        if not os.path.exists(voice_path):
            logger.warning(f"âŒ éŸ³è‰² '{voice_id}' çš„æ–‡ä»¶æœªæ‰¾åˆ°: {voice_path}")
            return False
        
        try:
            # ä½¿ç”¨ CosyVoice çš„ add_zero_shot_spk æ–¹æ³•ç¼“å­˜éŸ³è‰²ç‰¹å¾
            if hasattr(self.model, 'add_zero_shot_spk'):
                self.model.add_zero_shot_spk(prompt_text, voice_path, voice_id)
            
            # ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜
            self.voice_cache[voice_id] = {
                "file": voice_path,
                "prompt_text": prompt_text,
                "description": description,
                "is_loaded": True
            }
            
            logger.info(f"âœ… éŸ³è‰² '{voice_id}' åŠ è½½æˆåŠŸ: {voice_file}")
            return True
            
        except Exception as e:
            logger.warning(f"âŒ éŸ³è‰² '{voice_id}' åŠ è½½å¤±è´¥: {e}")
            return False
    
    def get_voice(self, voice_id: str) -> Optional[Dict]:
        """
        è·å–æŒ‡å®šéŸ³è‰²ä¿¡æ¯
        
        Args:
            voice_id: éŸ³è‰² ID
        
        Returns:
            éŸ³è‰²ä¿¡æ¯å­—å…¸,ä¸å­˜åœ¨åˆ™è¿”å› None
        """
        return self.voice_cache.get(voice_id)
    
    def list_voices(self) -> list:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨éŸ³è‰²
        
        Returns:
            éŸ³è‰²ä¿¡æ¯åˆ—è¡¨
        """
        return [
            {
                "id": voice_id,
                "file": os.path.basename(info["file"]),
                "prompt_text": info["prompt_text"],
                "description": info.get("description", ""),
                "is_loaded": info.get("is_loaded", False)
            }
            for voice_id, info in self.voice_cache.items()
        ]
    
    def get_default_voice(self) -> Optional[Dict]:
        """è·å–é»˜è®¤éŸ³è‰²"""
        return self.get_voice(self.default_voice_id)


def load_cosyvoice_model(
    model_dir: str = None,
    device: str = "cuda",
    fp16: bool = None,
    use_vllm: bool = None
) -> AutoModel:
    """
    åŠ è½½ CosyVoice æ¨¡å‹
    
    Args:
        model_dir: æ¨¡å‹ç›®å½•è·¯å¾„
        device: è¿è¡Œè®¾å¤‡
        fp16: æ˜¯å¦ä½¿ç”¨ FP16 æ¨ç†
        use_vllm: æ˜¯å¦å¯ç”¨ vLLM åŠ é€Ÿ
    
    Returns:
        åŠ è½½çš„æ¨¡å‹å®ä¾‹
    """
    global cosy_model, voice_cache_manager
    
    if cosy_model is not None:
        logger.info("æ¨¡å‹å·²åŠ è½½,è·³è¿‡é‡å¤åŠ è½½")
        return cosy_model
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼
    if model_dir is None:
        model_dir = settings.MODEL_DIR
    if fp16 is None:
        fp16 = settings.FP16
    if use_vllm is None:
        use_vllm = settings.USE_VLLM
    
    logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_dir}")
    logger.info(f"è®¾å¤‡: {device}, FP16: {fp16}, vLLMåŠ é€Ÿ: {use_vllm}")
    
    if use_vllm:
        try:
            import vllm
        except ImportError:
            logger.error("å¯ç”¨ vLLM å¤±è´¥: æœªæ‰¾åˆ° vllm åº“ã€‚è¯·å…ˆå®‰è£…: pip install vllm==0.9.0")
            sys.exit(1)
    
    start_time = time.time()
    
    try:
        cosy_model = AutoModel(
            model_dir=model_dir,
            load_trt=False,
            load_vllm=use_vllm,
            fp16=fp16
        )
    except TypeError as e:
        if "load_vllm" in str(e):
            logger.error("å½“å‰ CosyVoice ç‰ˆæœ¬ä¼¼ä¹ä¸æ”¯æŒ vLLM,è¯·ç¡®ä¿ä½¿ç”¨æœ€æ–°ä»£ç ")
        raise e
    
    logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆ,è€—æ—¶: {time.time() - start_time:.1f}s")
    logger.info(f"æ¨¡å‹é‡‡æ ·ç‡: {cosy_model.sample_rate}Hz, è¾“å‡ºé‡‡æ ·ç‡: {settings.OUTPUT_SAMPLE_RATE}Hz")
    
    # åˆå§‹åŒ–éŸ³è‰²ç¼“å­˜ç®¡ç†å™¨
    voice_cache_manager = VoiceCacheManager(cosy_model)
    voice_count = voice_cache_manager.load_voices()
    
    # æ¨¡å‹é¢„çƒ­
    if settings.ENABLE_MODEL_WARMUP:
        default_voice = voice_cache_manager.get_default_voice()
        if default_voice:
            warmup_model(
                prompt_wav_path=default_voice["file"],
                voice_id=settings.DEFAULT_VOICE_ID
            )
    
    return cosy_model


def warmup_model(prompt_wav_path: str = None, voice_id: str = None):
    """
    é¢„çƒ­æ¨¡å‹,å‡å°‘é¦–æ¬¡è¯·æ±‚å»¶è¿Ÿ
    
    Args:
        prompt_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
        voice_id: éŸ³è‰² ID
    """
    global cosy_model
    
    if cosy_model is None:
        logger.warning("æ¨¡å‹æœªåŠ è½½,è·³è¿‡é¢„çƒ­")
        return
    
    logger.info("ğŸ”¥ æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
    start_time = time.time()
    
    warmup_text = "é¢„çƒ­æµ‹è¯•"
    warmup_prompt_text = "You are a helpful assistant.<|endofprompt|>é¢„çƒ­"
    
    try:
        if prompt_wav_path and os.path.exists(prompt_wav_path):
            # ä½¿ç”¨ zero-shot æ¨¡å¼é¢„çƒ­
            spk_id = voice_id if voice_id else "default"
            
            for _ in cosy_model.inference_zero_shot(
                warmup_text,
                warmup_prompt_text,
                prompt_wav_path,
                stream=False,
                zero_shot_spk_id=spk_id
            ):
                pass
            
            logger.info(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ,è€—æ—¶: {time.time() - start_time:.1f}s")
        else:
            logger.info("â­ è·³è¿‡é¢„çƒ­ (æ— å‚è€ƒéŸ³é¢‘)")
            
    except Exception as e:
        logger.warning(f"é¢„çƒ­å¤±è´¥ (ä¸å½±å“æ­£å¸¸ä½¿ç”¨): {e}")


def get_cosy_model() -> Optional[AutoModel]:
    """è·å–å…¨å±€æ¨¡å‹å®ä¾‹"""
    return cosy_model


def get_voice_cache_manager() -> Optional[VoiceCacheManager]:
    """è·å–éŸ³è‰²ç¼“å­˜ç®¡ç†å™¨"""
    return voice_cache_manager


def get_inference_lock() -> threading.Lock:
    """è·å–æ¨ç†é”"""
    return inference_lock
